{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31677b61",
   "metadata": {},
   "source": [
    "\n",
    "# Machine learning tutorial\n",
    "# MLGWSC-1 kickoff meeting\n",
    "### 12. 10. 2021, Zoom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82bb8fe",
   "metadata": {},
   "source": [
    "## What is ML?\n",
    "**Tom M. Mitchell**: *A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8ed990",
   "metadata": {},
   "source": [
    "## Basic formulation of ML problem\n",
    "We need to approximate function $f^\\star : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m,~ f^\\star\\left(\\mathbf{x}\\right) = \\mathbf{y}$.\n",
    "\n",
    "Let us define a *model* $f : \\mathbb{R}^n \\times \\mathbb{R}^k \\rightarrow \\mathbb{R}^m,~ f\\left(\\mathbf{x}, \\theta\\right) = \\mathbf{y}$; typically: $\\mathbf{x}$ = inputs, $\\mathbf{y}$ = outputs, $\\theta$ = weights.\n",
    "\n",
    "If the model is suitable to the problem, then a set of weights $\\hat{\\theta}$ exists such that $f\\left(\\mathbf{x}, \\hat{\\theta}\\right) \\approx f^\\star\\left(\\mathbf{x}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fca1a8",
   "metadata": {},
   "source": [
    "### How to find the right weights?\n",
    "\n",
    "Two more necessary ingredients:\n",
    "\n",
    "Dataset: $\\mathbf{X}, \\mathbf{Y}, \\forall i: \\mathbf{Y}_i = f^\\star\\left(\\mathbf{X}_i\\right) + \\mathrm{noise}$, $\\mathbf{Y}$ are called labels\n",
    "\n",
    "Loss/cost/error function: $\\mathcal{C}\\left(\\mathbf{Y}, f\\left(\\mathbf{X}, \\theta\\right)\\right)$ to measure deviation of the model from the labels (e.g. mean squared error)\n",
    "\n",
    "Training the model: $\\hat{\\theta} = \\mathrm{argmin}_\\theta\\{\\mathcal{C}\\left(\\mathbf{Y}, f\\left(\\mathbf{X}, \\theta\\right)\\right)\\} \\rightarrow \\boxed{f\\left(\\mathbf{x}, \\hat{\\theta}\\right)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fa25c2",
   "metadata": {},
   "source": [
    "## Example: Polynomial regression\n",
    "\n",
    "We'll attempt to approximate the polynomial $f^\\star\\left(x\\right) = 2x - 10x^5 + 15x^{10}$ (*true model*) using polynomial regression: minimization of the mean squared error $\\mathcal{C}_{MSE}\\left(\\mathbf{Y}, f\\left(\\mathbf{X}, \\theta\\right)\\right) = \\frac{1}{N}\\sum_{i=0}^{N-1}\\left(\\mathbf{Y}_i - f\\left(\\mathbf{X}_i, \\theta\\right)\\right)^2$, and the weights are the coefficients of the polynomial.\n",
    "\n",
    "If the $\\mathbf{X}$ inputs are non-degenerate (no two are the same) and there are more than the polynomial degree, there is a single minimum and it is easy to compute.\n",
    "\n",
    "We'll sample some values of $f^\\star$ with a little white Gaussian noise as a training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc079367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "### Create the true model and some values to plot\n",
    "poly = np.polynomial.Polynomial([0., 2., 0., 0., 0., -10., 0., 0., 0., 0., 15.])\n",
    "plot_X = np.linspace(0., 1., 10**3)\n",
    "plot_Y = poly(plot_X)\n",
    "### Create the plot limits\n",
    "xlim = (0., 1.)\n",
    "ylim = (min(plot_Y), max(plot_Y))\n",
    "extend_factor = .1\n",
    "ylim = (ylim[0]-extend_factor*(ylim[1]-ylim[0]), ylim[1]+extend_factor*(ylim[1]-ylim[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f123193",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Generate training data\n",
    "# tr_X = rng.uniform(0., 1., 11)\n",
    "tr_X = np.linspace(0., 1., 11)\n",
    "tr_Y = poly(tr_X) + rng.normal(0., .3, len(tr_X))\n",
    "\n",
    "### Plot the true model and the training data\n",
    "plt.figure(figsize=(10., 5.))\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "plt.plot(plot_X, plot_Y)\n",
    "plt.scatter(tr_X, tr_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab099f7",
   "metadata": {},
   "source": [
    "Now, let's train some polynomials. Change the line that starts with `deg` to set the orders of fitting polynomials. One might expect that since we have the 11 points required to fit a 10-degree polynomial and the data was generated by a 10-degree polynomial, that would again reproduce the true model the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88c7050",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit polynomials\n",
    "degs = [1, 3, 5, 10]\n",
    "# degs = range(11)\n",
    "fit_polys = [poly.fit(tr_X, tr_Y, deg) for deg in degs]\n",
    "\n",
    "### Plot the fitted polynomials\n",
    "plt.figure(figsize=(15., 10.))\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "for deg, fit_poly in zip(degs, fit_polys):\n",
    "    plt.plot(plot_X, fit_poly(plot_X), label='%i'%deg)\n",
    "plt.scatter(tr_X, tr_Y, label='train')\n",
    "plt.plot(plot_X, plot_Y, label='orig')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da1c659",
   "metadata": {},
   "source": [
    "Generate some test data and evaluate the training (*in-sample*) and test (*out-of-sample*) losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8057aaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate test data, same distribution as training data\n",
    "te_X = rng.uniform(0., 1., 100)\n",
    "te_Y = poly(te_X) + rng.normal(0., .3, len(te_X))\n",
    "\n",
    "### Compute the loss values\n",
    "losses = []\n",
    "for deg, fit_poly in zip(degs, fit_polys):\n",
    "    new_losses = []\n",
    "    for X, Y in ((tr_X, tr_Y), (te_X, te_Y)):\n",
    "        model_Y = fit_poly(X)\n",
    "        new_losses.append(np.mean((model_Y - Y)**2))\n",
    "    losses.append(new_losses)\n",
    "losses = np.array(losses)\n",
    "\n",
    "### Plot the loss values\n",
    "plt.figure(figsize=(10., 5.))\n",
    "plt.plot(degs, losses[:, 0], '.-', label='training')\n",
    "plt.plot(degs, losses[:, 1], '.-', label='test')\n",
    "plt.semilogy()\n",
    "plt.ylim((1.e-5, 1.e1))\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6e34fc",
   "metadata": {},
   "source": [
    "In fact, the degree-10 polynomial is not a good fit. The plot above demonstrates that with a limited number of noisy data points, too high complexity model gives the best training loss (so it's a very good fit to that particular set of data), but has trouble generalizing to new data from the same underlying distribution. This phenomenon is called *overfitting*. It is the difference between fitting and predicting.\n",
    "\n",
    "One must therefore, when working with more complex data and models, be careful about model complexity - increasing it may lead to a better fit on training data and at the same time, to worse predictions on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450d34aa",
   "metadata": {},
   "source": [
    "## Gradient descent algorithms\n",
    "\n",
    "Polynomial regression was a simple example, which is easy to solve - the loss optimization has a single global minimum (under weak assumptions). With more complex ML models, this is no longer the case. Usually, gradient descent-based algorithms are used to find the minimum of $E\\left(\\theta\\right) = \\mathcal{C}\\left(\\mathbf{Y}, f\\left(\\mathbf{X}, \\theta\\right)\\right)$. These algorithms require an 'initial position' $\\theta_0$ and attempt to find the solution iteratively.\n",
    "\n",
    "Basic gradient descent:\n",
    "$$\\mathbf{v}_t = \\eta\\nabla_\\theta E\\left(\\theta_t\\right),~\\theta_{t+1} = \\theta_t - \\mathbf{v}_t$$\n",
    "\n",
    "Let's try on a simple parabolic surface $E\\left(\\theta\\right) = \\theta_0^2 + \\theta_1^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaa8790",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parabolic surface and its gradient\n",
    "def surface(theta):\n",
    "    return np.sum(theta**2, axis=0)\n",
    "\n",
    "def gradient(theta):\n",
    "    return 2*theta\n",
    "\n",
    "### Generate contour plot data\n",
    "X = np.linspace(-5., 5., 1000)\n",
    "Y = np.linspace(-5., 5., 1000)\n",
    "Z = surface(np.stack((np.tile(np.expand_dims(X, 1), (1, len(Y))), np.tile(np.expand_dims(Y, 0), (len(X), 1))), axis=0)).T\n",
    "\n",
    "### Initial position and GD parameters\n",
    "theta = [np.array((-4., -2.))]\n",
    "epochs = range(21)\n",
    "eta = 0.1\n",
    "\n",
    "### GD loop\n",
    "for e in epochs[1:]:\n",
    "    v = eta*gradient(theta[-1])\n",
    "    theta.append(theta[-1]-v)\n",
    "theta = np.stack(theta, axis=1)\n",
    "\n",
    "### Plot results\n",
    "plt.figure(figsize=(7., 7.))\n",
    "plt.contour(X, Y, Z)\n",
    "plt.plot(theta[0], theta[1], '.-', linewidth=1., markersize=10.)\n",
    "plt.axis('square')\n",
    "plt.xlim((min(X), max(X)))\n",
    "plt.ylim((min(Y), max(Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ef48c0",
   "metadata": {},
   "source": [
    "To be able to work with complex data, we need complex models - so we need the datasets to be large. Computation of the gradient of $E\\left(\\theta\\right)$ over the full training dataset is then computationally very difficult, let alone doing several tens or hundreds of optimization steps, so we need to apply one more approximation to our method.\n",
    "\n",
    "We assume that evaluating the loss gradient over a smaller subset of the training dataset is representative of the full computation. We split the training dataset into **batches** and do a single step of the gradient descent method on each of these batches. One iteration over all the batches is called a **training epoch**. This method is called **stochastic gradient descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60357337",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "Neural networks are inspired by the human neural system. The basic element is a very simple **artificial neuron**, which merely takes a linear combination of its input values, with an additional constant term, and applies a non-linear function:\n",
    "$$y(\\mathbf{x}) = \\sigma\\left(\\sum_{i} w_ix_i + b\\right).$$\n",
    "\n",
    "The $w_i$ values are usually called **weights** and $b$ is the **bias**. The function $\\sigma$ is called the **activation function** and there are many different choices. Some of the most popular are, for example:\n",
    "$$\\mathrm{Sigmoid}\\left(x\\right) = \\frac{1}{1+e^{-x}},~\\mathrm{ReLU}\\left(x\\right) = \\mathrm{max}\\left(0, x\\right)~.$$\n",
    "\n",
    "For classification problems, the *Softmax* is very popular, because it maps $\\mathbb{R}^N$ to a set of positive numbers that sum up to one. This is often used to represent probabilities.\n",
    "$$\\mathrm{Softmax}\\left(\\mathbf{x}\\right)_i = \\frac{\\exp\\left(x_i\\right)}{\\sum_j \\exp\\left(x_j\\right)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e31ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the Softmax and ReLU activation functions\n",
    "X = np.linspace(-10., 10., 10000)\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10., 5.))\n",
    "\n",
    "Y = 1./(1.+np.exp(-X))\n",
    "axes[0].plot(X, Y, label='Sigmoid')\n",
    "axes[0].legend(loc='best')\n",
    "\n",
    "Y = np.amax(np.stack((X, np.zeros_like(X)), axis=0), axis=0)\n",
    "axes[1].plot(X, Y, label='ReLU')\n",
    "axes[1].legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73e15d2",
   "metadata": {},
   "source": [
    "We can order multiple independent neurons into a *layer*, so that:\n",
    "$$y_i\\left(\\mathbf{x}\\right) = \\sigma\\left(\\sum_j w_{ij}x_j + b_i\\right).$$\n",
    "\n",
    "Let us apply this to a real-life classification problem. We will use the MNIST dataset, which contains 60 000 annotated hand-written digits as single-channel $28\\times 28$ images and 10 000 more for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ab7e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "fname = 'MNIST'\n",
    "\n",
    "### Download and initialize datasets\n",
    "TrainDS_orig = torchvision.datasets.MNIST(fname, train=True, download=True)\n",
    "TestDS_orig = torchvision.datasets.MNIST(fname, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9540119",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Plot examples\n",
    "fig, axes = plt.subplots(nrows=2, ncols=6, figsize=(15., 6.))\n",
    "for axes_row in axes:\n",
    "    for ax in axes_row:\n",
    "        test_index = rng.integers(0, len(TestDS_orig))\n",
    "        image, orig_label = TestDS_orig[test_index]\n",
    "        ax.set_axis_off()\n",
    "        ax.imshow(image)\n",
    "        ax.set_title('True: %i' % orig_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a301cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the label transform from an integer to a set of probabilities\n",
    "def target_transform(inlabel):\n",
    "    newlabel = torch.zeros(10)\n",
    "    newlabel[inlabel] = 1.\n",
    "    return newlabel\n",
    "\n",
    "### Reinitialize datasets with the transforms\n",
    "TrainDS = torchvision.datasets.MNIST(fname, train=True, download=True,\n",
    "            target_transform=target_transform, transform=torchvision.transforms.ToTensor())\n",
    "TestDS = torchvision.datasets.MNIST(fname, train=False,\n",
    "            target_transform=target_transform, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "### Initialize DataLoaders as PyTorch convenience\n",
    "TrainDL = torch.utils.data.DataLoader(TrainDS, shuffle=True, batch_size=32)\n",
    "TestDL = torch.utils.data.DataLoader(TestDS, batch_size=1000)\n",
    "\n",
    "### Choose device: 'cuda' or 'cpu'\n",
    "device = 'cuda:0'\n",
    "\n",
    "### Define the dense neuron layer\n",
    "Network = torch.nn.Sequential(\n",
    "    torch.nn.Flatten(),            # 28x28 -> 784\n",
    "    torch.nn.Linear(28**2, 10),    # 784 -> 10\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")\n",
    "Network.to(device=device)\n",
    "\n",
    "### Get information about model\n",
    "totpars = 0\n",
    "for par in Network.parameters():\n",
    "    newpars = 1\n",
    "    for num in par.shape:\n",
    "        newpars *= num\n",
    "    totpars += newpars\n",
    "print(Network)\n",
    "print('%i trainable parameters' % totpars)\n",
    "\n",
    "### Initialize loss function and optimizer\n",
    "crit = torch.nn.BCELoss()\n",
    "opt = torch.optim.SGD(Network.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4627d06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set model in training mode and create the epochs axis\n",
    "Network.train()\n",
    "epochs = range(1, 11)\n",
    "\n",
    "### Train the model\n",
    "for e in epochs:\n",
    "    tr_loss = 0.\n",
    "    samples = 0\n",
    "    ### Loop over batches\n",
    "    for inputs, labels in TrainDL:\n",
    "        opt.zero_grad() # zero gradient values\n",
    "        inputs = inputs.to(device=device) # move input and label tensors to the device with the model\n",
    "        labels = labels.to(device=device)\n",
    "        outputs = Network(inputs) # compute model outputs\n",
    "        loss = crit(outputs, labels) # compute batch loss\n",
    "        loss.backward() # back-propagate the gradients\n",
    "        opt.step() # update the model weights\n",
    "        tr_loss += loss.clone().cpu().item()*len(inputs) # add the batch loss to the running loss\n",
    "        samples += len(inputs) # update the number of processed samples\n",
    "    tr_loss /= samples # compute training loss\n",
    "    print(e, tr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28df487b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set model in evaluation mode\n",
    "Network.eval()\n",
    "\n",
    "### Compute the test loss\n",
    "with torch.no_grad():\n",
    "    te_loss = 0.\n",
    "    samples = 0\n",
    "    accuracy = 0\n",
    "    ### Loop over batches\n",
    "    for inputs, labels in TestDL:\n",
    "        inputs = inputs.to(device=device)\n",
    "        labels = labels.to(device=device)\n",
    "        outputs = Network(inputs)\n",
    "        loss = crit(outputs, labels)\n",
    "        te_loss += loss.clone().cpu().item()*len(inputs)\n",
    "        accuracy += torch.sum(torch.eq(torch.max(labels, 1)[1], torch.max(outputs, 1)[1]), dtype=int).clone().cpu().item()\n",
    "        samples += len(inputs)\n",
    "    te_loss /= samples\n",
    "    accuracy /= samples\n",
    "    print('Test loss: %f, accuracy: %f' % (te_loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d116381",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Draw some random images from the test dataset and compare the true labels to the network outputs\n",
    "fig, axes = plt.subplots(nrows=2, ncols=6, figsize=(15., 6.))\n",
    "### Loop over subplots\n",
    "for axes_row in axes:\n",
    "    for ax in axes_row:\n",
    "        ### Draw the images\n",
    "        test_index = rng.integers(0, len(TestDS))\n",
    "        sample, label = TestDS[test_index]\n",
    "        image, orig_label = TestDS_orig[test_index]\n",
    "        ### Show image\n",
    "        ax.set_axis_off()\n",
    "        ax.imshow(image)\n",
    "        ### Compute the predictions\n",
    "        with torch.no_grad():\n",
    "            output = Network(torch.unsqueeze(sample, dim=0).to(device=device))\n",
    "            certainty, output = torch.max(output[0], 0)\n",
    "            certainty = certainty.clone().cpu().item()\n",
    "            output = output.clone().cpu().item()\n",
    "        ax.set_title('True: %i, predicted: %i\\nat %f' % (orig_label, output, certainty))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c81b8ca",
   "metadata": {},
   "source": [
    "## Deep neural networks\n",
    "\n",
    "This is already a simple neural network. However, to build a **deep neural network**, we need to stack several such layers, such that each subsequent layer takes the output of the previous layer as its input. This method of stacking allows us to build very complex models - while the individual structural elements are very simple, the non-linear elements weaved into the network of linear operations allow the model to approximate any function to an arbitrary degree as long as you're flexible on depth and layer size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2799df",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show schematic of a fully connected deep neural network\n",
    "import matplotlib.image as mpimg\n",
    "fig, ax = plt.subplots(figsize=(15., 7.))\n",
    "ax.set_axis_off()\n",
    "ax.imshow(mpimg.imread('nn.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eb653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define a simple two-layer network\n",
    "Network = torch.nn.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(28**2, 100),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(100, 10),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")\n",
    "Network.to(device=device)\n",
    "\n",
    "### Get information about model\n",
    "totpars = 0\n",
    "for par in Network.parameters():\n",
    "    newpars = 1\n",
    "    for num in par.shape:\n",
    "        newpars *= num\n",
    "    totpars += newpars\n",
    "print(Network)\n",
    "print('%i trainable parameters' % totpars)\n",
    "\n",
    "### Initialize loss function and optimizer\n",
    "crit = torch.nn.BCELoss()\n",
    "opt = torch.optim.SGD(Network.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6079df",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set model in training mode and create the epochs axis\n",
    "Network.train()\n",
    "epochs = range(1, 11)\n",
    "\n",
    "### Train the model\n",
    "for e in epochs:\n",
    "    tr_loss = 0.\n",
    "    samples = 0\n",
    "    ### Loop over batches\n",
    "    for inputs, labels in TrainDL:\n",
    "        opt.zero_grad() # zero gradient values\n",
    "        inputs = inputs.to(device=device) # move input and label tensors to the device with the model\n",
    "        labels = labels.to(device=device)\n",
    "        outputs = Network(inputs) # compute model outputs\n",
    "        loss = crit(outputs, labels) # compute batch loss\n",
    "        loss.backward() # back-propagate the gradients\n",
    "        opt.step() # update the model weights\n",
    "        tr_loss += loss.clone().cpu().item()*len(inputs) # add the batch loss to the running loss\n",
    "        samples += len(inputs) # update the number of processed samples\n",
    "    tr_loss /= samples # compute training loss\n",
    "    print(e, tr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f621710",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set model in evaluation mode\n",
    "Network.eval()\n",
    "\n",
    "### Compute the test loss\n",
    "with torch.no_grad():\n",
    "    te_loss = 0.\n",
    "    samples = 0\n",
    "    accuracy = 0\n",
    "    ### Loop over batches\n",
    "    for inputs, labels in TestDL:\n",
    "        inputs = inputs.to(device=device)\n",
    "        labels = labels.to(device=device)\n",
    "        outputs = Network(inputs)\n",
    "        loss = crit(outputs, labels)\n",
    "        te_loss += loss.clone().cpu().item()*len(inputs)\n",
    "        accuracy += torch.sum(torch.eq(torch.max(labels, 1)[1], torch.max(outputs, 1)[1]), dtype=int).clone().cpu().item()\n",
    "        samples += len(inputs)\n",
    "    te_loss /= samples\n",
    "    accuracy /= samples\n",
    "    print('Test loss: %f, accuracy: %f' % (te_loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10980d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Draw some random images from the test dataset and compare the true labels to the network outputs\n",
    "fig, axes = plt.subplots(nrows=2, ncols=6, figsize=(15., 6.))\n",
    "### Loop over subplots\n",
    "for axes_row in axes:\n",
    "    for ax in axes_row:\n",
    "        ### Draw the images\n",
    "        test_index = rng.integers(0, len(TestDS))\n",
    "        sample, label = TestDS[test_index]\n",
    "        image, orig_label = TestDS_orig[test_index]\n",
    "        ### Show image\n",
    "        ax.set_axis_off()\n",
    "        ax.imshow(image)\n",
    "        ### Compute the predictions\n",
    "        with torch.no_grad():\n",
    "            output = Network(torch.unsqueeze(sample, dim=0).to(device=device))\n",
    "            certainty, output = torch.max(output[0], 0)\n",
    "            certainty = certainty.clone().cpu().item()\n",
    "            output = output.clone().cpu().item()\n",
    "        ax.set_title('True: %i, predicted: %i\\nat %f' % (orig_label, output, certainty))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6160517a",
   "metadata": {},
   "source": [
    "## Convolutional neural networks\n",
    "\n",
    "A slightly more convoluted (sorry) type of neural networks has been inspired by the animal visual cortex. Convolutional layers are introduced, which slide *filters* over an input image, producing another image - often with a larger number of channels, so these can be hard to visualize. These are used to build **convolutional neural networks**, which typically consist of a convolutional part and a dense part. The convolutional part is constructed out of alternating convolutional layers and pooling layers (these downsample the image between the convolutional layers), the result is flattened and passed to a dense part, which is typically a deep neural network as shown before.\n",
    "\n",
    "This structure removes some less crucial connections between neurons and use many weights for multiple connections as opposed to a fully connected network. This allows us to build a network just as suited to some problems as a fully connected network while dramatically reducing the number of free parameters to be optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d149fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15., 7.))\n",
    "ax.set_axis_off()\n",
    "ax.imshow(mpimg.imread('cnn.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9fffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a simple convolutional neural network\n",
    "Network = torch.nn.Sequential(      #  1x28x28\n",
    "    torch.nn.Conv2d(1, 12, (9, 9)),  #  12x20x20\n",
    "    torch.nn.MaxPool2d((2, 2)),     #  12x10x10\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(12, 24, (5, 5)), # 24x 6x 6\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d((3, 3)),     # 24x 2x 2\n",
    "    torch.nn.Flatten(),             #       96\n",
    "    torch.nn.Linear(96, 16),        #       16\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(16, 10),        #       10\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")\n",
    "Network.to(device=device)\n",
    "\n",
    "### Get information about model\n",
    "totpars = 0\n",
    "for par in Network.parameters():\n",
    "    newpars = 1\n",
    "    for num in par.shape:\n",
    "        newpars *= num\n",
    "    totpars += newpars\n",
    "print(Network)\n",
    "print('%i trainable parameters' % totpars)\n",
    "\n",
    "### Initialize loss function and optimizer\n",
    "opt = torch.optim.SGD(Network.parameters(), lr=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1ae9e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Set model in training mode and create the epochs axis\n",
    "Network.train()\n",
    "epochs = range(1, 11)\n",
    "\n",
    "### Train the model\n",
    "for e in epochs:\n",
    "    tr_loss = 0.\n",
    "    samples = 0\n",
    "    ### Loop over batches\n",
    "    for inputs, labels in TrainDL:\n",
    "        opt.zero_grad() # zero gradient values\n",
    "        inputs = inputs.to(device=device) # move input and label tensors to the device with the model\n",
    "        labels = labels.to(device=device)\n",
    "        outputs = Network(inputs) # compute model outputs\n",
    "        loss = crit(outputs, labels) # compute batch loss\n",
    "        loss.backward() # back-propagate the gradients\n",
    "        opt.step() # update the model weights\n",
    "        tr_loss += loss.clone().cpu().item()*len(inputs) # add the batch loss to the running loss\n",
    "        samples += len(inputs) # update the number of processed samples\n",
    "    tr_loss /= samples # compute training loss\n",
    "    print(e, tr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc1f564",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set model in evaluation mode\n",
    "Network.eval()\n",
    "\n",
    "### Compute the test loss\n",
    "with torch.no_grad():\n",
    "    te_loss = 0.\n",
    "    samples = 0\n",
    "    accuracy = 0\n",
    "    ### Loop over batches\n",
    "    for inputs, labels in TestDL:\n",
    "        inputs = inputs.to(device=device)\n",
    "        labels = labels.to(device=device)\n",
    "        outputs = Network(inputs)\n",
    "        loss = crit(outputs, labels)\n",
    "        te_loss += loss.clone().cpu().item()*len(inputs)\n",
    "        accuracy += torch.sum(torch.eq(torch.max(labels, 1)[1], torch.max(outputs, 1)[1]), dtype=int).clone().cpu().item()\n",
    "        samples += len(inputs)\n",
    "    te_loss /= samples\n",
    "    accuracy /= samples\n",
    "    print('Test loss: %f, accuracy: %f' % (te_loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b659e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Draw some random images from the test dataset and compare the true labels to the network outputs\n",
    "fig, axes = plt.subplots(nrows=2, ncols=6, figsize=(15., 6.))\n",
    "### Loop over subplots\n",
    "for axes_row in axes:\n",
    "    for ax in axes_row:\n",
    "        ### Draw the images\n",
    "        test_index = rng.integers(0, len(TestDS))\n",
    "        sample, label = TestDS[test_index]\n",
    "        image, orig_label = TestDS_orig[test_index]\n",
    "        ### Show image\n",
    "        ax.set_axis_off()\n",
    "        ax.imshow(image)\n",
    "        ### Compute the predictions\n",
    "        with torch.no_grad():\n",
    "            output = Network(torch.unsqueeze(sample, dim=0).to(device=device))\n",
    "            certainty, output = torch.max(output[0], 0)\n",
    "            certainty = certainty.clone().cpu().item()\n",
    "            output = output.clone().cpu().item()\n",
    "        ax.set_title('True: %i, predicted: %i\\nat %f' % (orig_label, output, certainty))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
